import os
import smtplib
import feedparser
import time
import urllib.parse
import urllib.request
import re
import requests # ì›¹í˜ì´ì§€ ì ‘ì†ìš©
from bs4 import BeautifulSoup # HTML ë³¸ë¬¸ ì¶”ì¶œìš©
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime
import google.generativeai as genai

# --- í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (GitHub Secrets) ---
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")
EMAIL_SENDER = os.environ.get("EMAIL_SENDER")
EMAIL_PASSWORD = os.environ.get("EMAIL_PASSWORD")
EMAIL_RECEIVERS = os.environ.get("EMAIL_RECEIVERS")

# --- ì„¤ì •: í‚¤ì›Œë“œ ë° í•„í„° ---
KEYWORDS = [
    "í¬ìŠ¤ì½”ì´ì•¤ì”¨", 
    "ê±´ì„¤ ì›ìì¬ ê°€ê²©", 
    "ê³µì •ìœ„ í•˜ë„ê¸‰ ê±´ì„¤", 
    "ê±´ì„¤ ì¤‘ëŒ€ì¬í•´ì²˜ë²Œë²•",
    "ê±´ì„¤ì‚¬ í˜‘ë ¥ì‚¬ ESG",
    "ì£¼ìš” ê±´ì„¤ì‚¬ êµ¬ë§¤ ë™í–¥",
    "ê±´ì„¤ ìì¬ í™˜ìœ¨ ìœ ê°€",
    "í•´ìƒ ìš´ì„ SCFI ê±´ì„¤",
    "ìŠ¤ë§ˆíŠ¸ ê±´ì„¤ ëª¨ë“ˆëŸ¬ OSC",
    "ê±´ì„¤ í˜„ì¥ ì¸ë ¥ë‚œ ì™¸êµ­ì¸",
    "ê±´ì„¤ ë…¸ì¡° íŒŒì—… ë…¸ë€ë´‰íˆ¬ë²•",
    "ë‚©í’ˆëŒ€ê¸ˆ ì—°ë™ì œ ê±´ì„¤",
    "ê±´ì„¤ì‚°ì—…ê¸°ë³¸ë²• ê°œì •",
    "í™”ë¬¼ì—°ëŒ€ ë ˆë¯¸ì½˜ ìš´ì†¡ íŒŒì—…"
]

# ì£¼ì‹/íˆ¬ì ê´€ë ¨ ë…¸ì´ì¦ˆ ì œê±°ë¥¼ ìœ„í•œ ê¸ˆì§€ì–´ ëª©ë¡
EXCLUDE_KEYWORDS = [
    "íŠ¹ì§•ì£¼", "í…Œë§ˆì£¼", "ê´€ë ¨ì£¼", "ì£¼ê°€", "ê¸‰ë“±", "ê¸‰ë½", "ìƒí•œê°€", "í•˜í•œê°€",
    "ê±°ë˜ëŸ‰", "ë§¤ìˆ˜", "ë§¤ë„", "ëª©í‘œê°€", "ì²´ê²°", "ì¦ì‹œ", "ì¢…ëª©", "íˆ¬ìì",
    "ì§€ìˆ˜", "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ë§ˆê°"
]

def get_korea_time():
    """ì„œë²„ ì‹œê°„(UTC)ì„ í•œêµ­ ì‹œê°„(KST)ìœ¼ë¡œ ë³€í™˜"""
    utc_now = datetime.now(timezone.utc)
    kst_now = utc_now + timedelta(hours=9)
    return kst_now

def is_stock_noise(title):
    """ì œëª©ì— ì£¼ì‹ ê´€ë ¨ ê¸ˆì§€ì–´ê°€ ìˆëŠ”ì§€ ê²€ì‚¬"""
    for bad_word in EXCLUDE_KEYWORDS:
        if bad_word in title:
            return True
    return False

def is_recent(published_str):
    """ë‰´ìŠ¤ ë‚ ì§œê°€ 24ì‹œê°„ ì´ë‚´ì¸ì§€ í™•ì¸"""
    if not published_str: return False
    try:
        pub_date = parsedate_to_datetime(published_str)
        if pub_date.tzinfo:
            pub_date = pub_date.astimezone(timezone.utc)
        else:
            pub_date = pub_date.replace(tzinfo=timezone.utc)
        
        now_utc = datetime.now(timezone.utc)
        one_day_ago = now_utc - timedelta(hours=24)
        return pub_date > one_day_ago
    except:
        return True

def fetch_article_content(url):
    """ë§í¬ë¥¼ íƒ€ê³  ë“¤ì–´ê°€ì„œ ê¸°ì‚¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê¸ì–´ì˜´ (ìŠ¤í¬ë©ìš©)"""
    try:
        # êµ¬ê¸€ ë‰´ìŠ¤ ë¦¬ë‹¤ì´ë ‰íŠ¸ ë“±ì„ í†µê³¼í•˜ê¸° ìœ„í•œ í—¤ë” ë³´ê°•
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        
        # Session ì‚¬ìš©ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ ì¶”ì  ëŠ¥ë ¥ í–¥ìƒ
        session = requests.Session()
        # íƒ€ì„ì•„ì›ƒ 10ì´ˆë¡œ ë„‰ë„‰í•˜ê²Œ ì„¤ì •
        response = session.get(url, headers=headers, timeout=10, allow_redirects=True)
        response.encoding = response.apparent_encoding # í•œê¸€ ê¹¨ì§ ë°©ì§€
        
        # êµ¬ê¸€ ë‰´ìŠ¤ ê¸°ë³¸ í˜ì´ì§€ê°€ ê¸í˜”ëŠ”ì§€ í™•ì¸ (ì‹¤íŒ¨ë¡œ ê°„ì£¼)
        if "Comprehensive up-to-date news coverage" in response.text:
            return None

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±° (ê´‘ê³ , ë©”ë‰´, ìŠ¤í¬ë¦½íŠ¸ ë“±)
            for element in soup(["script", "style", "header", "footer", "nav", "aside", "iframe", "form"]):
                element.decompose()

            content = ""
            
            # 1. <article> íƒœê·¸ ìš°ì„  ê²€ìƒ‰
            article = soup.find('article')
            if article:
                content = article.get_text(strip=True, separator='\n')
            else:
                # 2. ë³¸ë¬¸ìœ¼ë¡œ ì¶”ì •ë˜ëŠ” <div>ë‚˜ <p> íƒœê·¸ ìˆ˜ì§‘
                # ì£¼ìš” ì–¸ë¡ ì‚¬ë³„ ë³¸ë¬¸ í´ë˜ìŠ¤ëª… íŒ¨í„´ ì‹œë„
                target_divs = soup.find_all('div', class_=re.compile(r'(article|content|body|detail)', re.I))
                if target_divs:
                    # ê°€ì¥ í…ìŠ¤íŠ¸ê°€ ê¸´ div ì„ íƒ
                    best_div = max(target_divs, key=lambda x: len(x.get_text()))
                    content = best_div.get_text(strip=True, separator='\n')
                else:
                    # ìµœí›„ì˜ ìˆ˜ë‹¨: ëª¨ë“  <p> íƒœê·¸ ìˆ˜ì§‘
                    paragraphs = soup.find_all('p')
                    # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸í•˜ê³  í•©ì¹˜ê¸°
                    content = "\n".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            
            # í…ìŠ¤íŠ¸ ì •ì œ (ì—°ì†ëœ ê³µë°±/ì¤„ë°”ê¿ˆ ì œê±°)
            content = re.sub(r'\n\s*\n', '\n\n', content)
            
            # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ êµ¬ê¸€ ì•ˆë‚´ ë¬¸êµ¬ë©´ ì‹¤íŒ¨ ì²˜ë¦¬
            if len(content) < 50 or "Comprehensive up-to-date" in content:
                return None

            return content[:1500] + "..." if len(content) > 1500 else content # ìµœëŒ€ 1500ì
    except Exception as e:
        print(f"    - Scraping Error: {e}")
        return None
    
    return None

def fetch_news():
    """RSS ë‰´ìŠ¤ ìˆ˜ì§‘ + ë³¸ë¬¸ ìŠ¤í¬ë©"""
    news_items = []
    print("ğŸ” ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    
    for keyword in KEYWORDS:
        # ê²€ìƒ‰ì–´ ë’¤ì— '-ì£¼ì‹ -ì¢…ëª©' ë“±ì„ ë¶™ì—¬ì„œ êµ¬ê¸€ ê²€ìƒ‰ ë‹¨ê³„ì—ì„œë„ 1ì°¨ í•„í„°ë§
        negative_query = " -ì£¼ì‹ -ì¢…ëª© -í…Œë§ˆ -íŠ¹ì§•ì£¼"
        encoded_query = urllib.parse.quote(f"{keyword}{negative_query} when:1d")
        url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
        
        try:
            feed = feedparser.parse(url)
            
            if not feed.entries and hasattr(feed, 'bozo_exception'): pass

            for entry in feed.entries[:3]:
                if is_recent(entry.published):
                    # 2ì°¨ í•„í„°ë§: ì œëª©ì— ê¸ˆì§€ì–´ í¬í•¨ ì—¬ë¶€ í™•ì¸
                    if is_stock_noise(entry.title):
                        continue

                    if not any(item['link'] == entry.link for item in news_items):
                        # RSSì— ê¸°ë³¸ì ìœ¼ë¡œ í¬í•¨ëœ ìš”ì•½ë¬¸(summary) ê°€ì ¸ì˜¤ê¸° (HTML íƒœê·¸ ì œê±°)
                        rss_summary = ""
                        if hasattr(entry, 'summary'):
                             rss_summary = BeautifulSoup(entry.summary, "html.parser").get_text(strip=True)
                        elif hasattr(entry, 'description'):
                             rss_summary = BeautifulSoup(entry.description, "html.parser").get_text(strip=True)

                        print(f"  Scraping: {entry.title[:10]}...")
                        # ë³¸ë¬¸ ìŠ¤í¬ë© ì‹œë„
                        scraped_text = fetch_article_content(entry.link)
                        
                        # ìŠ¤í¬ë© ì„±ê³µí•˜ë©´ ë³¸ë¬¸ ì‚¬ìš©, ì‹¤íŒ¨í•˜ë©´ RSS ìš”ì•½ë¬¸ ì‚¬ìš©
                        final_text = scraped_text if scraped_text else f"(ë³¸ë¬¸ ìˆ˜ì§‘ ë¶ˆê°€ - ìš”ì•½ë³¸ ì œê³µ)\n\n{rss_summary}"
                        
                        if not final_text.strip():
                            final_text = "(ë³¸ë¬¸ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§í¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.)"

                        news_items.append({
                            "title": entry.title,
                            "link": entry.link,
                            "keyword": keyword,
                            "date": entry.published,
                            "full_text": final_text # ë³¸ë¬¸ ì €ì¥
                        })
        except Exception as e:
            print(f"âš ï¸ '{keyword}' ì˜¤ë¥˜: {e}")
            continue
            
    print(f"âœ… ì´ {len(news_items)}ê°œì˜ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ.")
    return news_items

def generate_report(news_items):
    """Gemini AI ë¦¬í¬íŠ¸ (ë‚ ì§œ ê°•ì œ ì£¼ì…)"""
    if not news_items: return None
    
    kst_now = get_korea_time()
    today_formatted = kst_now.strftime("%Yë…„ %mì›” %dì¼") # ì˜ˆ: 2025ë…„ 05ì›” 20ì¼
    
    print("ğŸ§  AI ë¶„ì„ ì‹œì‘...")
    try:
        genai.configure(api_key=GOOGLE_API_KEY)
        model = genai.GenerativeModel('gemini-2.5-flash-preview-09-2025')

        news_text = ""
        for idx, item in enumerate(news_items):
            news_text += f"[{idx+1}] {item['title']} ({item['keyword']})\n"

        # í”„ë¡¬í”„íŠ¸ì— ë‚ ì§œì™€ ë¶€ì„œëª…ì„ ëª…í™•íˆ ë°•ì•„ë„£ìŒ
        prompt = f"""
        ì˜¤ëŠ˜ì€ {today_formatted}ì…ë‹ˆë‹¤.
        ë‹¹ì‹ ì€ **í¬ìŠ¤ì½”ì´ì•¤ì”¨ êµ¬ë§¤ê³„ì•½ì‹¤**ì˜ ìˆ˜ì„ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
        ì•„ë˜ ë‰´ìŠ¤ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ 'Daily Market & Risk Briefing' ì´ë©”ì¼ì„ ì‘ì„±í•˜ì„¸ìš”.

        [ë‰´ìŠ¤ ëª©ë¡]
        {news_text}

        [ì‘ì„± ì›ì¹™]
        1. **ë‚ ì§œ ì¤€ìˆ˜**: ë°˜ë“œì‹œ ì˜¤ëŠ˜ ë‚ ì§œ({today_formatted})ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. 2024ë…„ ë“± ê³¼ê±° ì—°ë„ í‘œê¸° ê¸ˆì§€.
        2. **ì£¼ì‹/íˆ¬ì ë°°ì œ**: ê±´ì„¤ í…Œë§ˆì£¼, ì£¼ê°€ ë“±ë½ ë‚´ìš©ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        3. **êµ¬ë§¤ê³„ì•½ì‹¤ ê´€ì **: ê³„ì•½, ë‚©ê¸°, ë‹¨ê°€, ë²•ì  ë¦¬ìŠ¤í¬ ìœ„ì£¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.

        [ë³´ê³ ì„œ í˜•ì‹ (HTML)]
        - `<div>` íƒœê·¸ë¡œ ê°ì‹¸ì„œ ì‘ì„±.
        - **[ì˜¤ëŠ˜ì˜ ì‹œì¥ ë‚ ì”¨]**: â˜€ï¸/â˜ï¸/â˜” ì•„ì´ì½˜ ì‚¬ìš©í•˜ì—¬ 1ì¤„ ìš”ì•½.
        - **ë¶„ì•¼ë³„ ë‰´ìŠ¤**: 
          - [ê·œì œ/ë¦¬ìŠ¤í¬], [ìì¬/ì‹œí™©], [ê¸€ë¡œë²Œ/ë¬¼ë¥˜] ë“±ìœ¼ë¡œ ë¶„ë¥˜.
          - ê° ê¸°ì‚¬ í•˜ë‹¨ì— `ğŸ’¡Insight: (ë‚´ìš©)` í•œ ì¤„ ì¶”ê°€.
        """
        
        response = model.generate_content(prompt)
        return response.text.replace("```html", "").replace("```", "")
    except Exception as e:
        print(f"âŒ AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

# --- PDF ìŠ¤í¬ë©ë¶ ìƒì„± (ë³¸ë¬¸ í¬í•¨) ---
def create_scrap_pdf(news_items):
    print("ğŸ“„ ìŠ¤í¬ë© PDF ìƒì„± ì‹œì‘...")
    try:
        from fpdf import FPDF
    except ImportError:
        print("âŒ fpdf2 ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    font_path = 'NanumGothic.ttf'
    if not os.path.exists(font_path):
        urllib.request.urlretrieve("https://raw.githubusercontent.com/google/fonts/main/ofl/nanumgothic/NanumGothic-Regular.ttf", font_path)

    pdf = FPDF()
    pdf.add_font('Nanum', '', font_path) # í°íŠ¸ ë“±ë¡ ë¨¼ì €!
    pdf.add_page()

    kst_now = get_korea_time()
    date_str = kst_now.strftime("%Yë…„ %mì›” %dì¼")

    # íƒ€ì´í‹€
    pdf.set_font('Nanum', size=20)
    pdf.cell(0, 15, f'êµ¬ë§¤ê³„ì•½ì‹¤ ì¼ì¼ ë‰´ìŠ¤ ìŠ¤í¬ë© ({date_str})', ln=True, align='C')
    pdf.ln(10)

    # ë‰´ìŠ¤ ë£¨í”„
    for idx, item in enumerate(news_items):
        # ê¸°ì‚¬ ì œëª©
        pdf.set_font('Nanum', size=14)
        pdf.set_text_color(0, 84, 166) # í¬ìŠ¤ì½” ë¸”ë£¨
        pdf.multi_cell(0, 8, f"{idx+1}. {item['title']}")
        
        # ë©”íƒ€ ì •ë³´
        pdf.set_font('Nanum', size=9)
        pdf.set_text_color(100, 100, 100)
        pdf.cell(0, 6, f"í‚¤ì›Œë“œ: {item['keyword']} | ë§í¬: {item['link'][:50]}...", ln=True, link=item['link'])
        pdf.ln(2)

        # ê¸°ì‚¬ ë³¸ë¬¸ (ìŠ¤í¬ë© ë‚´ìš©)
        pdf.set_font('Nanum', size=10)
        pdf.set_text_color(30, 30, 30)
        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì •ë¦¬ (ì¤„ë°”ê¿ˆ ë“±)
        body_text = item.get('full_text', 'ë‚´ìš© ì—†ìŒ').replace('\t', '  ')
        # í•œê¸€ í°íŠ¸ì—ì„œ ìœ ë‹ˆì½”ë“œ ë¬¸ìê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê¸°ë³¸ ì •ì œ
        body_text = body_text.encode('latin-1', 'replace').decode('latin-1') # fpdf ì¸ì½”ë”© ì—ëŸ¬ ë°©ì§€ìš© (ì‹¬í”Œ ì²˜ë¦¬)
        # ì‹¤ì œë¡œëŠ” fpdf2ê°€ ìœ ë‹ˆì½”ë“œë¥¼ ê½¤ ì˜ ì²˜ë¦¬í•˜ì§€ë§Œ, ì•ˆì „ì¥ì¹˜ë¡œ íŠ¹ìˆ˜ë¬¸ì ì¼ë¶€ ì œì™¸
        
        pdf.multi_cell(0, 5, body_text)
        
        # ê¸°ì‚¬ ê°„ êµ¬ë¶„ì„ 
        pdf.ln(5)
        pdf.set_draw_color(200, 200, 200)
        pdf.line(10, pdf.get_y(), 200, pdf.get_y())
        pdf.ln(10)

    filename = f"News_Scrap_{kst_now.strftime('%Y%m%d')}.pdf"
    pdf.output(filename)
    return filename

def send_email(html_body, pdf_file=None):
    if not html_body: return

    kst_now = get_korea_time()
    today_str = kst_now.strftime("%Yë…„ %mì›” %dì¼")
    subject = f"[Daily] {today_str} êµ¬ë§¤ê³„ì•½ì‹¤ ì‹œì¥ ë™í–¥ ë³´ê³ "
    
    # ê¹”ë”í•œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ HTML (Original Style)
    full_html = f"""
    <html>
    <body style="font-family: 'Malgun Gothic', sans-serif; color: #333; line-height: 1.6;">
        <div style="padding: 20px; border: 1px solid #ddd;">
            <h2 style="color: #0054a6; margin-bottom: 20px;">POSCO E&C êµ¬ë§¤ê³„ì•½ì‹¤ Daily Briefing</h2>
            <p>ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ, êµ¬ë§¤ê³„ì•½ì‹¤ ì—¬ëŸ¬ë¶„.<br>
            {today_str} ì£¼ìš” ì‹œì¥ ë™í–¥ì…ë‹ˆë‹¤.</p>
            
            <div style="background-color: #f9f9f9; padding: 15px; border-left: 5px solid #0054a6; margin: 20px 0;">
                <strong>ğŸ“‚ ìœ ì²¨:</strong> ê¸ˆì¼ ì£¼ìš” ê¸°ì‚¬ ì „ë¬¸ ìŠ¤í¬ë© (PDF)
            </div>

            <hr style="border:0; border-top:1px solid #eee; margin: 20px 0;">
            
            {html_body}
            
            <hr style="border:0; border-top:1px solid #eee; margin: 20px 0;">
            <p style="font-size: 12px; color: #888;">* ë³¸ ë©”ì¼ì€ AI Agentê°€ ìë™ ë°œì†¡í–ˆìŠµë‹ˆë‹¤.</p>
        </div>
    </body>
    </html>
    """

    msg = MIMEMultipart()
    msg['From'] = EMAIL_SENDER
    msg['To'] = EMAIL_RECEIVERS
    msg['Subject'] = subject
    msg.attach(MIMEText(full_html, 'html'))

    if pdf_file and os.path.exists(pdf_file):
        with open(pdf_file, "rb") as f:
            attach = MIMEApplication(f.read(), _subtype="pdf")
            attach.add_header('Content-Disposition', 'attachment', filename=pdf_file)
            msg.attach(attach)

    try:
        server = smtplib.SMTP('smtp.gmail.com', 587)
        server.starttls()
        server.login(EMAIL_SENDER, EMAIL_PASSWORD)
        receivers = [r.strip() for r in EMAIL_RECEIVERS.split(',')]
        server.sendmail(EMAIL_SENDER, receivers, msg.as_string())
        server.quit()
        print(f"ğŸ“§ ë°œì†¡ ì„±ê³µ: {len(receivers)}ëª…ì—ê²Œ ì „ì†¡ ì™„ë£Œ.")
    except Exception as e:
        print(f"âŒ ë°œì†¡ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    if not GOOGLE_API_KEY:
        print("âŒ API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        items = fetch_news()
        if items:
            report_html = generate_report(items)
            pdf_filename = create_scrap_pdf(items) # ìŠ¤í¬ë© ì „ìš© PDF ìƒì„±
            
            if report_html:
                send_email(report_html, pdf_filename)
            else:
                print("âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨")
        else:
            print("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
